import json
import os
from datetime import datetime, timezone, timedelta

import requests
import feedparser
from langdetect import detect, LangDetectException

# --------- è¯­è¨€æ£€æµ‹ + ç¿»è¯‘å‡½æ•° ---------


def detect_src_lang(text: str) -> str:
    """
    ä½¿ç”¨ langdetect è‡ªåŠ¨æ£€æµ‹åŸæ–‡è¯­è¨€ã€‚
    è¿”å›ç±»ä¼¼: 'en', 'pt', 'id', 'ja', 'ko', 'ar', 'hi' ç­‰ã€‚
    å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œé»˜è®¤å½“æˆè‹±æ–‡ã€‚
    """
    try:
        code = detect(text)
    except LangDetectException:
        return "en"

    # langdetect è¿”å› zh-cn / zh-tw ç­‰ï¼Œç»Ÿä¸€æˆ zh
    if code.startswith("zh"):
        return "zh"
    return code


def translate_to_zh(text: str) -> str:
    """
    è‡ªåŠ¨è¯†åˆ«åŸæ–‡è¯­è¨€ -> ç¿»è¯‘æˆä¸­æ–‡ã€‚
    å·²ç»æ˜¯ä¸­æ–‡çš„å°±ç›´æ¥è¿”å›åŸæ–‡ã€‚
    """
    if not text:
        return text

    src_lang = detect_src_lang(text)

    # å·²ç»æ˜¯ä¸­æ–‡å°±ä¸ç”¨ç¿»è¯‘
    if src_lang == "zh":
        return text

    # MyMemory ç”¨çš„è¯­è¨€ä»£ç å¤§è‡´å…¼å®¹ ISO-639-1ï¼Œ
    # å°è¯­ç§ä¸è®¤è¯†æ—¶æˆ‘ä»¬å…œåº•å½“æˆè‹±æ–‡ã€‚
    supported = {
        "en",
        "pt",
        "es",
        "fr",
        "de",
        "it",
        "ja",
        "ko",
        "id",
        "hi",
        "ar",
        "ru",
    }
    if src_lang not in supported:
        src_lang = "en"

    try:
        url = (
            "https://api.mymemory.translated.net/get"
            f"?q={requests.utils.quote(text)}&langpair={src_lang}|zh-CN"
        )
        r = requests.get(url, timeout=10).json()
        translated = r.get("responseData", {}).get("translatedText", "")
        return translated or text
    except Exception:
        # ç¿»è¯‘å¤±è´¥å°±ç”¨åŸæ–‡å…œåº•
        return text


# --------- å„ç«™ç‚¹æŠ“å–å‡½æ•° ---------

API_HN = "https://hn.algolia.com/api/v1/search?tags=front_page"


def fetch_hn(limit=15, region="Global"):
    """Hacker News é¦–é¡µ"""
    resp = requests.get(API_HN, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    items = []
    for hit in data.get("hits", [])[:limit]:
        title_en = hit.get("title") or hit.get("story_title") or "No title"
        title_zh = translate_to_zh(title_en)
        url = hit.get("url") or hit.get("story_url") or ""
        points = hit.get("points") or 0
        comments = hit.get("num_comments") or 0
        items.append(
            {
                "source": "Hacker News",
                "title": title_en,      # åŸæ–‡
                "title_zh": title_zh,   # ä¸­æ–‡ç¿»è¯‘
                "url": url,
                "points": points,
                "comments": comments,
                "region": region,
                "published": None,
                "summary": "",
            }
        )
    return items


def fetch_rss(url, source_name, region="Global", limit=10):
    """é€šç”¨ RSS æŠ“å– + è‡ªåŠ¨è¯†åˆ«åŸæ–‡è¯­è¨€"""
    feed = feedparser.parse(url)
    items = []

    for entry in feed.entries[:limit]:
        title_raw = entry.get("title", "").strip()
        link = entry.get("link", "") or ""
        title_zh = translate_to_zh(title_raw) if title_raw else ""

        items.append(
            {
                "source": source_name,
                "title": title_raw,
                "title_zh": title_zh,
                "url": link,          # å’Œ HN ä¿æŒä¸€è‡´ï¼Œéƒ½ç”¨ url å­—æ®µ
                "region": region,     # åœ°åŒºæ ‡ç­¾
                "published": entry.get("published", ""),
                "summary": entry.get("summary", "").strip(),
            }
        )
    return items


def main():
    all_items = []

    # ========= ğŸŒ Global / å…¨çƒ =========
    all_items += fetch_hn(limit=15, region="Global")

    all_items += fetch_rss(
        "https://www.reddit.com/r/all/.rss",
        "Reddit r/all",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.reddit.com/r/worldnews/.rss",
        "Reddit r/worldnews",
        region="Global",
        limit=10,
    )

    all_items += fetch_rss(
        "https://techcrunch.com/feed/",
        "TechCrunch",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.theverge.com/rss/index.xml",
        "The Verge",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.producthunt.com/feed",
        "Product Hunt",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "http://feeds.bbci.co.uk/news/world/rss.xml",
        "BBC World",
        region="Global",
        limit=10,
    )

    # ========= ğŸ‡§ğŸ‡· Brazil / å·´è¥¿ =========
    all_items += fetch_rss(
        "https://feeds.folha.uol.com.br/emcimadahora/rss091.xml",
        "Folha de S.Paulo",
        region="Brazil",
        limit=10,
    )
    all_items += fetch_rss(
        "https://riotimesonline.com/feed/",
        "The Rio Times",
        region="Brazil",
        limit=10,
    )

    # ========= ğŸ‡®ğŸ‡© Indonesia / å°å°¼ =========
    all_items += fetch_rss(
        "https://rss.thejakartapost.com/home",
        "The Jakarta Post",
        region="Indonesia",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.kontan.co.id/feed",
        "Kontan",
        region="Indonesia",
        limit=10,
    )

    # ========= ğŸ‡®ğŸ‡³ India / å°åº¦ =========
    all_items += fetch_rss(
        "https://timesofindia.indiatimes.com/rssfeedstopstories.cms",
        "Times of India - Top Stories",
        region="India",
        limit=10,
    )
    all_items += fetch_rss(
        "https://feeds.feedburner.com/ndtvnews-top-stories",
        "NDTV - Top Stories",
        region="India",
        limit=10,
    )

    # ========= ğŸ‡¯ğŸ‡µ Japan / æ—¥æœ¬ =========
    all_items += fetch_rss(
    "https://www3.nhk.or.jp/rss/news/cat0.xml",
    "NHK News",
    region="Japan",
    limit=10,
)
    
    # ========= ğŸ‡°ğŸ‡· South Korea / éŸ©å›½ =========
    # The Korea Heraldï¼ˆè‹±æ–‡ï¼Œå·²åœ¨ä½¿ç”¨ï¼‰
    all_items += fetch_rss(
        "https://www.koreaherald.com/rss",
        "The Korea Herald",
        region="South Korea",
        limit=10,
    )

    # The Korea Timesï¼ˆè‹±æ–‡ï¼‰
    all_items += fetch_rss(
        "https://www.koreatimes.co.kr/www/rss/nation.xml",
        "The Korea Times",
        region="South Korea",
        limit=10,
    )

    # Yonhap News / éŸ©è”ç¤¾ï¼ˆè‹±æ–‡ç‰ˆ Top Newsï¼‰
    all_items += fetch_rss(
        "https://en.yna.co.kr/feed/topnews.xml",
        "Yonhap News",
        region="South Korea",
        limit=10,
    )

    # ========= ğŸ‡¸ğŸ‡¦ Saudi Arabia / æ²™ç‰¹ =========
    all_items += fetch_rss(
        "https://www.arabnews.com/rss",
        "Arab News",
        region="Saudi Arabia",
        limit=10,
    )
    all_items += fetch_rss(
        "https://saudigazette.com.sa/rssFeed/74",
        "Saudi Gazette",
        region="Saudi Arabia",
        limit=10,
    )

    # åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)

    out = {
        "last_updated": beijing_time.strftime("%Y-%m-%d %H:%M:%S"),
        "items": all_items,
    }

    os.makedirs("data", exist_ok=True)
    with open("data/news.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
