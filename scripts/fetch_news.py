import json
import os
from datetime import datetime, timezone, timedelta

import requests
import feedparser

# --------- ç¿»è¯‘å‡½æ•° ---------


def translate_to_zh(text: str) -> str:
    """ä½¿ç”¨ MyMemory å…è´¹ APIï¼ŒæŠŠè‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡"""
    try:
        url = (
            "https://api.mymemory.translated.net/get"
            f"?q={requests.utils.quote(text)}&langpair=en|zh-CN"
        )
        r = requests.get(url, timeout=10).json()
        translated = r.get("responseData", {}).get("translatedText", "")
        return translated or text
    except Exception:
        # ç¿»è¯‘å¤±è´¥å°±ç”¨åŸæ–‡å…œåº•
        return text


# --------- å„ç«™ç‚¹æŠ“å–å‡½æ•° ---------

API_HN = "https://hn.algolia.com/api/v1/search?tags=front_page"


def fetch_hn(limit=15, region="Global"):
    """Hacker News é¦–é¡µ"""
    resp = requests.get(API_HN, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    items = []
    for hit in data.get("hits", [])[:limit]:
        title_en = hit.get("title") or hit.get("story_title") or "No title"
        title_zh = translate_to_zh(title_en)
        url = hit.get("url") or hit.get("story_url") or ""
        points = hit.get("points") or 0
        comments = hit.get("num_comments") or 0
        items.append(
            {
                "source": "Hacker News",
                "title": title_en,      # è‹±æ–‡åŸæ–‡
                "title_zh": title_zh,   # ä¸­æ–‡ç¿»è¯‘
                "url": url,
                "points": points,
                "comments": comments,
                "region": region,
                "published": None,
                "summary": "",
            }
        )
    return items


def fetch_rss(url, source_name, region="Global", limit=10):
    """é€šç”¨ RSS æŠ“å–"""
    feed = feedparser.parse(url)
    items = []
    for entry in feed.entries[:limit]:
        title_en = entry.get("title", "").strip()
        title_zh = translate_to_zh(title_en) if title_en else ""
        link = entry.get("link", "")

        items.append(
            {
                "source": source_name,
                "title": title_en,
                "title_zh": title_zh,
                "url": link,          # å’Œ Hacker News ä¿æŒä¸€è‡´ï¼Œéƒ½ç”¨ url å­—æ®µ
                "region": region,     # åœ°åŒºæ ‡ç­¾
                "published": entry.get("published", ""),
                "summary": entry.get("summary", "").strip(),
            }
        )
    return items


def main():
    all_items = []

    # ========= ğŸŒ Global / å…¨çƒ =========
    all_items += fetch_hn(limit=15, region="Global")

    all_items += fetch_rss(
        "https://www.reddit.com/r/all/.rss",
        "Reddit r/all",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.reddit.com/r/worldnews/.rss",
        "Reddit r/worldnews",
        region="Global",
        limit=10,
    )

    all_items += fetch_rss(
        "https://techcrunch.com/feed/",
        "TechCrunch",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.theverge.com/rss/index.xml",
        "The Verge",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.producthunt.com/feed",
        "Product Hunt",
        region="Global",
        limit=10,
    )
    all_items += fetch_rss(
        "http://feeds.bbci.co.uk/news/world/rss.xml",
        "BBC World",
        region="Global",
        limit=10,
    )
    # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œç»§ç»­åŠ  NYTimes World ç­‰å…¶å®ƒ Global æº

    # ========= ğŸ‡§ğŸ‡· Brazil / å·´è¥¿ =========
    all_items += fetch_rss(
        "https://feeds.folha.uol.com.br/emcimadahora/rss091.xml",
        "Folha de S.Paulo",
        region="Brazil",
        limit=10,
    )
    all_items += fetch_rss(
        "https://riotimesonline.com/feed/",
        "The Rio Times",
        region="Brazil",
        limit=10,
    )

    # ========= ğŸ‡®ğŸ‡© Indonesia / å°å°¼ =========
    all_items += fetch_rss(
        "https://rss.thejakartapost.com/home",
        "The Jakarta Post",
        region="Indonesia",
        limit=10,
    )
    all_items += fetch_rss(
        "https://www.kontan.co.id/feed",
        "Kontan",
        region="Indonesia",
        limit=10,
    )

    # ========= ğŸ‡®ğŸ‡³ India / å°åº¦ =========
    all_items += fetch_rss(
        "https://timesofindia.indiatimes.com/rssfeedstopstories.cms",
        "Times of India - Top Stories",
        region="India",
        limit=10,
    )
    all_items += fetch_rss(
        "https://feeds.feedburner.com/ndtvnews-top-stories",
        "NDTV - Top Stories",
        region="India",
        limit=10,
    )

    # ========= ğŸ‡¯ğŸ‡µ Japan / æ—¥æœ¬ =========
    all_items += fetch_rss(
        "https://feedx.net/rss/nhk.xml",
        "NHK WORLD-JAPAN",
        region="Japan",
        limit=10,
    )

    # ========= ğŸ‡°ğŸ‡· South Korea / éŸ©å›½ =========
    all_items += fetch_rss(
        "https://www.koreaherald.com/rss",
        "The Korea Herald",
        region="South Korea",
        limit=10,
    )

    # ========= ğŸ‡¸ğŸ‡¦ Saudi Arabia / æ²™ç‰¹ =========
    all_items += fetch_rss(
        "https://www.arabnews.com/rss",
        "Arab News",
        region="Saudi Arabia",
        limit=10,
    )
    all_items += fetch_rss(
        "https://saudigazette.com.sa/rssFeed/74",
        "Saudi Gazette",
        region="Saudi Arabia",
        limit=10,
    )

    # åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)

    out = {
        "last_updated": beijing_time.strftime("%Y-%m-%d %H:%M:%S"),
        "items": all_items,
    }

    os.makedirs("data", exist_ok=True)
    with open("data/news.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
